{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Software Development Capability Analysis\n",
    "## by Marc Vitalis\n",
    "\n",
    "## Preliminary Wrangling\n",
    "\n",
    "> Briefly introduce your dataset here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all packages and set plots to be embedded inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Load in your dataset and describe its properties through the questions below.\n",
    "Try and motivate your exploration goals through this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workitems = pd.read_csv('workitems_master.csv')\n",
    "workitems.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workitems.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert Dates to Date type**\n",
    "\n",
    "Date format are represented as string (object), we should change them first to datetime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workitems.new = pd.to_datetime(workitems.new)\n",
    "workitems.doing = pd.to_datetime(workitems.doing)\n",
    "workitems.done = pd.to_datetime(workitems.done)\n",
    "\n",
    "workitems.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workitems.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert Releases to Numeric**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workitems.loc[workitems.sprint.isna(), 'sprint'] = 0\n",
    "workitems.sprint = workitems.sprint.astype(int)\n",
    "workitems.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert Category Types**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workitems.workitem_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workitem_types = CategoricalDtype(categories = ['Story', 'Bug', 'Issue'], ordered=False)\n",
    "workitems.workitem_type = workitems.workitem_type.astype(workitem_types)\n",
    "workitems.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workitems.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the structure of your dataset?\n",
    "\n",
    "> The dataset consists of 2393, with 10 features (workitem_type, estimate, words, rel (release), sprint, assigned_to, new (date started), doing (date started working), done, and actual work (done - doing). Variables main point of interest are the date stamps for the work. Some are just to describe the work item such as sprint, release and assigned_to.\n",
    "\n",
    "### What is/are the main feature(s) of interest in your dataset?\n",
    "\n",
    "> I'm more interested how variables affects `actual_work`. The goal is find out for the patterns that affects the actual work.\n",
    "\n",
    "### What features in the dataset do you think will help support your investigation into your feature(s) of interest?\n",
    "\n",
    "> The dataset contains data and underwent to three (3) SDLC pattern (non-structured, semi-agile, scrum). The date stamps are very important (`new`, `doing`, `done`), this will help me extract important information, such as days of the week, months, or observe the time flow pattern if the SDLC pattern improves through time, or made it worst. As bonus I can also make use the correlation of titles to the actual work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Exploration\n",
    "\n",
    "> First to explore is the main point of interest, `actual_work`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binsize = 2\n",
    "bins = np.arange(0, workitems.actual_work.max()+binsize, binsize)\n",
    "\n",
    "plt.figure(figsize=[8, 5])\n",
    "plt.hist(data = workitems, x = 'actual_work', bins = bins)\n",
    "plt.xlabel('Actual Work (Days)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a long tail on the right, but most data are clumped between 1 to 20. We'll try log scale to spread out these clump a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try log scale since it has a long tail\n",
    "log_binsize = 0.1\n",
    "bins = 10 ** np.arange(0, np.log10(workitems['actual_work'].max())+log_binsize, log_binsize)\n",
    "\n",
    "plt.figure(figsize=[8, 5])\n",
    "plt.hist(data = workitems, x = 'actual_work', bins = bins)\n",
    "plt.xscale('log')\n",
    "plt.xticks([2, 5, 8, 13, 20, 40, 50, 100, 200], [2, 5, 8, 13, 20, 40, 50, 100, 200])\n",
    "plt.xlabel('Actual Work (Days)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph looks trimodal, with peaks on 20, 5 and large numbers on 1. Let's examine what the averages tells us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[workitems.actual_work.mean(), workitems.actual_work.median(), workitems.actual_work.mode()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have different data here. Mean, says a workitem can be done 9 days, average. This is hardly conclusive as we have work items that's pulling the value up. Median however, makes a bit more sense as a normal workitem is normally done 3 days. 1 has more occurrence, but since this data is not categorical, it just provides a bit of information about the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based from the information gathered from the team, one day of work are mostly possible for what they call `Bug Fest`. More exploring on the relationship of the `actual_work` and `workitem_type`. For now, let's investigate the distribution of the `workitem_type`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_color = sb.color_palette()[0]\n",
    "sb.countplot(data = workitems, x = 'workitem_type', color = base_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a big number of bugs in comparison to stories. Which makes sense for the spike in 1 day `actual_work` in the data. Let's explore the data further by looking into number of workitems being worked on per month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_month = workitems.sort_values(['doing'])\n",
    "work_month['month'] = work_month.doing.dt.strftime('%b %Y')\n",
    "\n",
    "plt.figure(figsize=(10, 15))\n",
    "sb.countplot(data = work_month, y = 'month', color = base_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the date has become more recent, it's getting more consistent on the number of workitems. We don't know yet the distribution of how many of these are stories, bugs or issues. More on that later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With scrums, there's a normal trend that during Fridays, there's a spike of things suddenly getting done. Let's investigate the distribution on weekdays, both on `doing` and `done`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekdays_type = CategoricalDtype(categories=['Monday' , 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'], ordered=True)\n",
    "\n",
    "work_wk_doing = workitems\n",
    "\n",
    "work_wk_doing['week'] = workitems.doing.dt.strftime('%A').astype(weekdays_type)\n",
    "\n",
    "sb.countplot(data = work_wk_doing, x = 'week', color = base_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_wk_done = workitems\n",
    "\n",
    "work_wk_done['week'] = workitems.done.dt.strftime('%A').astype(weekdays_type)\n",
    "\n",
    "sb.countplot(data = work_wk_done, x = 'week', color = base_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that the team is more productive when fresh during `Mondays` and tends to slow down throughout the week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's observe the workitems per sprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO remove nan in sprint and change to int\n",
    "work_sprint = workitems\n",
    "work_sprint.loc[work_sprint.rel.isna(), 'rel'] = '0'\n",
    "work_sprint = work_sprint.sort_values(['rel', 'sprint'])\n",
    "\n",
    "work_sprint['rel_sprint'] = work_sprint.rel + '/' + work_sprint.sprint.astype(str).str.pad(width = 3, side = 'left', fillchar = '0')\n",
    "\n",
    "plt.figure(figsize=(10, 15))\n",
    "sb.countplot(data = work_sprint, y = 'rel_sprint', color = base_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The work items in each releases vary. Although there are items that slotted in Sprint zero which means there might be error in input on those."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next to investigate is the estimates provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binsize = 4\n",
    "bins = np.arange(0, workitems.estimate.max()+binsize, binsize)\n",
    "\n",
    "plt.figure(figsize=[8, 5])\n",
    "plt.hist(data = workitems, x = 'estimate', bins = bins)\n",
    "plt.xlabel('Points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, it has a long tail. Let's try plotting the log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try log scale since it has a long tail\n",
    "log_binsize = 0.1\n",
    "bins = 10 ** np.arange(0, np.log10(workitems['estimate'].max())+log_binsize, log_binsize)\n",
    "\n",
    "plt.figure(figsize=[8, 5])\n",
    "plt.hist(data = workitems, x = 'estimate', bins = bins)\n",
    "plt.xscale('log')\n",
    "plt.xticks([2, 5, 8, 13, 20, 40, 50, 100, 200], [2, 5, 8, 13, 20, 40, 50, 100, 200])\n",
    "plt.xlabel('points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution has its peak between 8 to 13 estimates. Based on my conversation with the team, this is the 'just enough' size of stories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last to explore are the title broken down into words. Let's find out the top 40 word occurence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_words = workitems[['id', 'words']]\n",
    "\n",
    "work_words = work_words[['id', 'words']].words.str.split(',').apply(pd.Series) \\\n",
    "    .merge(work_words[['id', 'words']], right_index = True, left_index = True) \\\n",
    "    .drop([\"words\"], axis = 1) \\\n",
    "    .melt(id_vars = ['id'], value_name = \"word\") \\\n",
    "    .drop(\"variable\", axis = 1) \\\n",
    "    .dropna()\n",
    "\n",
    "plt.figure(figsize=(10, 15))\n",
    "sb.countplot(data = work_words, y = 'word', color = base_color, order = work_words.word.value_counts().iloc[:40].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results is interesting. Top entry is error, which probably evident mostly on bug items. Other items in top 40 mostly can describe the application itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss the distribution(s) of your variable(s) of interest. Were there any unusual points? Did you need to perform any transformations?\n",
    "\n",
    "> Your answer here!\n",
    "\n",
    "### Of the features you investigated, were there any unusual distributions? Did you perform any operations on the data to tidy, adjust, or change the form of the data? If so, why did you do this?\n",
    "\n",
    "> Your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bivariate Exploration\n",
    "\n",
    "> In this section, investigate relationships between pairs of variables in your\n",
    "data. Make sure the variables that you cover here have been introduced in some\n",
    "fashion in the previous section (univariate exploration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Talk about some of the relationships you observed in this part of the investigation. How did the feature(s) of interest vary with other features in the dataset?\n",
    "\n",
    "> Your answer here!\n",
    "\n",
    "### Did you observe any interesting relationships between the other features (not the main feature(s) of interest)?\n",
    "\n",
    "> Your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Exploration\n",
    "\n",
    "> Create plots of three or more variables to investigate your data even\n",
    "further. Make sure that your investigations are justified, and follow from\n",
    "your work in the previous sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Talk about some of the relationships you observed in this part of the investigation. Were there features that strengthened each other in terms of looking at your feature(s) of interest?\n",
    "\n",
    "> Your answer here!\n",
    "\n",
    "### Were there any interesting or surprising interactions between features?\n",
    "\n",
    "> Your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> At the end of your report, make sure that you export the notebook as an\n",
    "html file from the `File > Download as... > HTML` menu. Make sure you keep\n",
    "track of where the exported file goes, so you can put it in the same folder\n",
    "as this notebook for project submission. Also, make sure you remove all of\n",
    "the quote-formatted guide notes like this one before you finish your report!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
