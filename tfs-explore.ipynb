{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Software Development Capability Analysis\n",
    "## by Marc Vitalis\n",
    "\n",
    "## Preliminary Wrangling\n",
    "\n",
    "> Briefly introduce your dataset here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all packages and set plots to be embedded inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Load in your dataset and describe its properties through the questions below.\n",
    "Try and motivate your exploration goals through this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workitems = pd.read_csv('workitems_master.csv')\n",
    "workitems.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workitems.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert Dates to Date type**\n",
    "\n",
    "Date format are represented as string (object), we should change them first to datetime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workitems.new = pd.to_datetime(workitems.new)\n",
    "workitems.doing = pd.to_datetime(workitems.doing)\n",
    "workitems.done = pd.to_datetime(workitems.done)\n",
    "\n",
    "workitems.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workitems.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert Releases to Numeric**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workitems.loc[workitems.sprint.isna(), 'sprint'] = 0\n",
    "workitems.sprint = workitems.sprint.astype(int)\n",
    "workitems.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert Category Types**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workitems.workitem_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workitem_types = CategoricalDtype(categories = ['Story', 'Bug', 'Issue'], ordered=False)\n",
    "workitems.workitem_type = workitems.workitem_type.astype(workitem_types)\n",
    "workitems.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workitems.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the structure of your dataset?\n",
    "\n",
    "> The dataset consists of 2393, with 10 features (workitem_type, estimate, words, rel (release), sprint, assigned_to, new (date started), doing (date started working), done, and actual work (done - doing). Variables main point of interest are the date stamps for the work. Some are just to describe the work item such as sprint, release and assigned_to.\n",
    "\n",
    "### What is/are the main feature(s) of interest in your dataset?\n",
    "\n",
    "> I'm more interested how variables affects `actual_work`. The goal is find out for the patterns that affects the actual work.\n",
    "\n",
    "### What features in the dataset do you think will help support your investigation into your feature(s) of interest?\n",
    "\n",
    "> The dataset contains data and underwent to three (3) SDLC pattern (non-structured, semi-agile, scrum). The date stamps are very important (`new`, `doing`, `done`), this will help me extract important information, such as days of the week, months, or observe the time flow pattern if the SDLC pattern improves through time, or made it worst. As bonus I can also make use the correlation of titles to the actual work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Exploration\n",
    "\n",
    "> First to explore is the main point of interest, `actual_work`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binsize = 1\n",
    "bins = np.arange(0, workitems.actual_work.max()+binsize, binsize)\n",
    "\n",
    "plt.figure(figsize=[8, 5])\n",
    "plt.hist(data = workitems, x = 'actual_work', bins = bins)\n",
    "plt.xlabel('Actual Work (Days)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a huge spike on the 0-1 area, it's unlikely to have a workitem with zero day done, that is considered as no effort. Let's tidy this a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zero sum should be converted to a day of work if they have worked on it at least 2h\n",
    "zero_work = workitems.actual_work == 0\n",
    "workitems.loc[zero_work, 'actual_work'] = 1\n",
    "\n",
    "#just remove the zero effort ones\n",
    "workitems = workitems[((workitems.done - workitems.doing) / pd.Timedelta(hours = 1)) >  2]\n",
    "workitems.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[8, 5])\n",
    "plt.hist(data = workitems, x = 'actual_work', bins = bins)\n",
    "plt.xlabel('Actual Work (Days)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still a huge spike in 1, and have a very long tail, let's redistribute it with log scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try log scale since it has a long tail\n",
    "log_binsize = 0.05\n",
    "bins = 10 ** np.arange(0, np.log10(workitems['actual_work'].max())+log_binsize, log_binsize)\n",
    "\n",
    "plt.figure(figsize=[8, 5])\n",
    "plt.hist(data = workitems, x = 'actual_work', bins = bins)\n",
    "plt.xscale('log')\n",
    "plt.xticks([2, 5, 8, 13, 20, 40, 50, 100, 200], [2, 5, 8, 13, 20, 40, 50, 100])\n",
    "plt.xlabel('Actual Work (Days)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have a big spike at one day work, and another clump in `20` days, somehow even distribution from `2-20`. On the later sprints, the team is now doing a 3 week scrum, which is somehow the same to 20 days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems unusual to have work items that's take more than 30 days. Also, there's a big value on workitems th Let's try to find out if they are outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to look at are the ones with value `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get items with actual_work = 1, and investigate the data\n",
    "ones = workitems[workitems.actual_work == 1]\n",
    "ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones.workitem_type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By visual investigation, most of the workitems with `actual_work == 1` are mostly bugs. Now this make sense as bugs usually are quick to fix. Let's get their actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's investigate those workitems with more than 30 days value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workitems[workitems.actual_work > 30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Items here are either from our work when we are still doing `waterfall`, or the newer ones, they are legitimate workitems that got back and forth in development, and some went to hiatus, and still they are valid data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find out the averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[workitems.actual_work.mean(), workitems.actual_work.median(), workitems.actual_work.mode()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have different data here. Mean, says a workitem can be done 9 days, average. This is hardly conclusive as we have work items that's pulling the value up. Median however, makes a bit more sense as a normal workitem is normally done 5 days. 1 has more occurrence, but since this data is not categorical, it just provides a bit of information about the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based from the information gathered from the team, one day of work are mostly possible for what they call `Bug Fest`. More exploring on the relationship of the `actual_work` and `workitem_type`. For now, let's investigate the distribution of the `workitem_type`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_color = sb.color_palette()[0]\n",
    "sb.countplot(data = workitems, x = 'workitem_type', color = base_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a big number of bugs in comparison to stories. Which makes sense for the spike in 1 day `actual_work` in the data. Let's explore the data further by looking into number of workitems being worked on per month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's extract the dates first\n",
    "workitems['doing_year'] = workitems.doing.dt.year\n",
    "workitems['done_year'] = workitems.done.dt.year\n",
    "workitems['new_year'] = workitems.new.dt.year\n",
    "\n",
    "workitems['doing_month'] = workitems.doing.dt.strftime('%b')\n",
    "workitems['done_month'] = workitems.done.dt.strftime('%b')\n",
    "workitems['new_month'] = workitems.new.dt.strftime('%b')\n",
    "\n",
    "workitems['doing_dow'] = workitems.doing.dt.strftime('%A')\n",
    "workitems['done_dow'] = workitems.done.dt.strftime('%A')\n",
    "workitems['new_dow'] = workitems.new.dt.strftime('%A')\n",
    "\n",
    "workitems['doing_my'] = workitems.doing.dt.strftime('%b %Y')\n",
    "workitems['done_my'] = workitems.done.dt.strftime('%b %Y')\n",
    "workitems['new_my'] = workitems.new.dt.strftime('%b %Y')\n",
    "\n",
    "weekdays_type = CategoricalDtype(categories=['Monday' , 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'], ordered=True)\n",
    "months_type = CategoricalDtype(categories=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'], ordered=True)\n",
    "\n",
    "workitems.doing_month = workitems.doing_month.astype(months_type)\n",
    "workitems.done_month = workitems.done_month.astype(months_type)\n",
    "workitems.new_month = workitems.new_month.astype(months_type)\n",
    "\n",
    "workitems.doing_dow = workitems.doing_dow.astype(weekdays_type)\n",
    "workitems.done_dow = workitems.done_dow.astype(weekdays_type)\n",
    "workitems.new_dow = workitems.new_dow.astype(weekdays_type)\n",
    "\n",
    "workitems.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_month = workitems.sort_values(['doing'])\n",
    "work_month['month'] = work_month.doing.dt.strftime('%b %Y')\n",
    "\n",
    "plt.figure(figsize=(10, 15))\n",
    "sb.countplot(data = work_month, y = 'month', color = base_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the date has become more recent, it's getting more consistent on the number of workitems. We don't know yet the distribution of how many of these are stories, bugs or issues. More on that later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With scrums, there's a normal trend that during Fridays, there's a spike of things suddenly getting done. Let's investigate the distribution on weekdays, both on `doing` and `done`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "work_wk_doing = workitems\n",
    "\n",
    "work_wk_doing['week'] = workitems.doing.dt.strftime('%A').astype(weekdays_type)\n",
    "\n",
    "sb.countplot(data = work_wk_doing, x = 'week', color = base_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_wk_done = workitems\n",
    "\n",
    "work_wk_done['week'] = workitems.done.dt.strftime('%A').astype(weekdays_type)\n",
    "\n",
    "sb.countplot(data = work_wk_done, x = 'week', color = base_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that the team is more productive when fresh during `Mondays`, finishing most items on `Tuesdays` and tends to slow down throughout the week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next we'll look at is the distribution for product when creating new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idea_month = workitems.sort_values(['new'])\n",
    "idea_month['month'] = idea_month.new.dt.strftime('%b %Y')\n",
    "\n",
    "plt.figure(figsize=(10, 15))\n",
    "sb.countplot(data = idea_month, y = 'month', color = base_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 occurences when stories are created in bulk, `April 2015`, `May - Oct 2016` and `Dec 2017`. These are interesting points to ask what happened during these events. Let's check which day of the week normally the Product Team mostly creates stories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_wk_idea = workitems\n",
    "\n",
    "work_wk_idea['week'] = workitems.new.dt.strftime('%A').astype(weekdays_type)\n",
    "\n",
    "sb.countplot(data = work_wk_idea, x = 'week', color = base_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a balance on when they add stories in the whole week, of course `Saturdays` and `Sundays` are holidays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's observe the workitems per sprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO remove nan in sprint and change to int\n",
    "work_sprint = workitems\n",
    "work_sprint.loc[work_sprint.rel.isna(), 'rel'] = '0'\n",
    "work_sprint = work_sprint.sort_values(['rel', 'sprint'])\n",
    "\n",
    "work_sprint['rel_sprint'] = work_sprint.rel + '/' + work_sprint.sprint.astype(str).str.pad(width = 3, side = 'left', fillchar = '0')\n",
    "\n",
    "plt.figure(figsize=(10, 15))\n",
    "sb.countplot(data = work_sprint, y = 'rel_sprint', color = base_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The work items in each releases vary. Although there are items that slotted in Sprint zero which means there might be error in input on those."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next to investigate is the estimates provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binsize = 4\n",
    "bins = np.arange(0, workitems.estimate.max()+binsize, binsize)\n",
    "\n",
    "plt.figure(figsize=[8, 5])\n",
    "plt.hist(data = workitems, x = 'estimate', bins = bins)\n",
    "plt.xlabel('Points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, it has a long tail. Let's try plotting the log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try log scale since it has a long tail\n",
    "log_binsize = 0.1\n",
    "bins = 10 ** np.arange(0, np.log10(workitems['estimate'].max())+log_binsize, log_binsize)\n",
    "\n",
    "plt.figure(figsize=[8, 5])\n",
    "plt.hist(data = workitems, x = 'estimate', bins = bins)\n",
    "plt.xscale('log')\n",
    "plt.xticks([2, 5, 8, 13, 20, 40, 50, 100, 200], [2, 5, 8, 13, 20, 40, 50, 100, 200])\n",
    "plt.xlabel('points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution has its peak between 8 to 13 estimates. Based on my conversation with the team, this is the 'just enough' size of stories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last to explore are the title broken down into words. Let's find out the top 40 word occurence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_words = workitems[['id', 'words']]\n",
    "\n",
    "work_words = work_words[['id', 'words']].words.str.split(',').apply(pd.Series) \\\n",
    "    .merge(work_words[['id', 'words']], right_index = True, left_index = True) \\\n",
    "    .drop([\"words\"], axis = 1) \\\n",
    "    .melt(id_vars = ['id'], value_name = \"word\") \\\n",
    "    .drop(\"variable\", axis = 1) \\\n",
    "    .dropna()\n",
    "\n",
    "plt.figure(figsize=(10, 15))\n",
    "sb.countplot(data = work_words, y = 'word', color = base_color, order = work_words.word.value_counts().iloc[:40].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results is interesting. Top entry is error, which probably evident mostly on bug items. Other items in top 40 mostly can describe the application itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss the distribution(s) of your variable(s) of interest. Were there any unusual points? Did you need to perform any transformations?\n",
    "\n",
    "The actual work contains `zero` days which means `no effort`. I recalculated the actual effort by getting what's normally considered as work which is `2 hours`, and considered that already as one day of work. There are also what seems to be outliers, with high value in `one-day work`, however, with investigation, I found out that these are mostly bugs which makes sense as bugs are usually easy to get done. On the high spectrum, I checked them online the patterns on why they have large values. Some of them are from our waterfall method which consists of large `stories` and took days to months just to finished. Some of the data are also difficult to resolve which spanned to multiple sprints. I feel that they are important part of our data for now.\n",
    "\n",
    "### Of the features you investigated, were there any unusual distributions? Did you perform any operations on the data to tidy, adjust, or change the form of the data? If so, why did you do this?\n",
    "\n",
    "I recalculated `actual_work` to weed out the `no effort` stories. The words field are comma-delimited string, was converted into it's own data frame so we can examine them individually. To investigate individual characteristics of the dates, i also extracted `year`, `month` and `dow` of `new`, `doing` and `done`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bivariate Exploration\n",
    "\n",
    "First let's check pairwise correlation between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workitems.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_vars = ['actual_work', 'estimate']\n",
    "categoric_vars = ['workitem_type', 'new_month', 'doing_month', 'done_month', 'new_dow', 'doing_dow', 'done_dow']\n",
    "\n",
    "sb.heatmap(workitems[numeric_vars].corr(), annot = True, fmt = '.3f', cmap = 'vlag_r', center = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sb.PairGrid(data = workitems, vars = numeric_vars)\n",
    "g = g.map_diag(plt.hist, bins = 10)\n",
    "g.map_offdiag(plt.scatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxgrid(x, y, **kwargs):\n",
    "    \"\"\" Quick hack for creating box plots with seaborn's PairGrid. \"\"\"\n",
    "    default_color = sb.color_palette()[0]\n",
    "    sb.boxplot(x, y, color = default_color)\n",
    "\n",
    "plt.figure(figsize = [10, 10])\n",
    "g = sb.PairGrid(data = workitems, y_vars = ['actual_work', 'estimate'], x_vars = categoric_vars,\n",
    "                height = 3, aspect = 1.5)\n",
    "g.map(boxgrid)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = [32, 24])\n",
    "\n",
    "\n",
    "ax = plt.subplot(3, 1, 1)\n",
    "df = workitems.sort_values(['new_year', 'new_month'])\n",
    "g = sb.countplot(data = df, x = 'new_my', hue = 'workitem_type', palette = 'Blues')\n",
    "ax.legend(loc = 1, ncol = 1)\n",
    "g.set_xticklabels(df.new_my.unique(), rotation=30)\n",
    "\n",
    "ax = plt.subplot(3, 1, 2)\n",
    "df = workitems.sort_values(['doing_year', 'doing_month'])\n",
    "g = sb.countplot(data = df, x = 'doing_my', hue = 'workitem_type', palette = 'Blues')\n",
    "ax.legend(loc = 1, ncol = 1) # re-arrange legend to reduce overlapping\n",
    "g.set_xticklabels(df.new_my.unique(), rotation=30)\n",
    "\n",
    "ax = plt.subplot(3, 1, 3)\n",
    "df = workitems.sort_values(['done_year', 'done_month'])\n",
    "g = sb.countplot(data = df, x = 'done_my', hue = 'workitem_type', palette = 'Greens')\n",
    "ax.legend(ncol = 1) # re-arrange legend to remove overlapping\n",
    "g.set_xticklabels(df.new_my.unique(), rotation=30)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Talk about some of the relationships you observed in this part of the investigation. How did the feature(s) of interest vary with other features in the dataset?\n",
    "\n",
    "> Your answer here!\n",
    "\n",
    "### Did you observe any interesting relationships between the other features (not the main feature(s) of interest)?\n",
    "\n",
    "> Your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Exploration\n",
    "\n",
    "> Create plots of three or more variables to investigate your data even\n",
    "further. Make sure that your investigations are justified, and follow from\n",
    "your work in the previous sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Talk about some of the relationships you observed in this part of the investigation. Were there features that strengthened each other in terms of looking at your feature(s) of interest?\n",
    "\n",
    "> Your answer here!\n",
    "\n",
    "### Were there any interesting or surprising interactions between features?\n",
    "\n",
    "> Your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> At the end of your report, make sure that you export the notebook as an\n",
    "html file from the `File > Download as... > HTML` menu. Make sure you keep\n",
    "track of where the exported file goes, so you can put it in the same folder\n",
    "as this notebook for project submission. Also, make sure you remove all of\n",
    "the quote-formatted guide notes like this one before you finish your report!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
